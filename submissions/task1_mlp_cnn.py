# -*- coding: utf-8 -*-
"""Task1-expriment-godwin.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/132CCCSsahOKAGSOF3zrhOnG7XdPYRivd
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models, regularizers

import numpy as np
import matplotlib.pyplot as plt

print("TensorFlow version:", tf.__version__)
print("keras", keras.__version__)

(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()
X_train_full = X_train_full/255.0
X_test = X_test/255.0

X_valid, X_train = X_train_full[:6000], X_train_full[6000:]
y_valid, y_train = y_train_full[:6000], y_train_full[6000:]

print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("Test:", X_test.shape, y_test.shape)

#mlp model
mlp_model = keras.Sequential([
    keras.layers.Flatten(input_shape=[28, 28]),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])

mlp_model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="adam",
    metrics=["accuracy"]
)


mlp_model.summary()

history_mlp = mlp_model.fit(
    X_train, y_train,
    epochs=30,
    validation_data=(X_valid, y_valid)
)

test_loss, test_accuracy = mlp_model.evaluate(X_test, y_test)
print(f"Test accuracy: {test_accuracy * 100:.2f}%")

X_train_cnn = X_train[..., np.newaxis]
X_valid_cnn = X_valid[..., np.newaxis]
X_test_cnn = X_test[..., np.newaxis]

cnn_model =  keras.Sequential([
    layers.Conv2D(32, (3, 3), activation="relu", padding="same", input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation="relu", padding="same"),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(10, activation="softmax")
])

cnn_model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

cnn_model.summary()

history_cnn = cnn_model.fit(
    X_train_cnn, y_train,
    epochs = 12,
    validation_data=(X_valid_cnn, y_valid)
)


test_loss_cnn, test_accuracy_cnn = cnn_model.evaluate(X_test_cnn, y_test)
print(f"Test accuracy (CNN): {test_accuracy_cnn:4f}")

from tensorflow.keras import regularizers
from tensorflow.keras.optimizers import Adam, SGD, RMSprop

def build_mlp(activation='relu', optimizer='adam', dropout_rate=0.0, l2_reg=0.0):
  model = keras.Sequential([
      layers.Flatten(input_shape=[28, 28]),
      layers.Dense(300, activation=activation,
                   kernel_regularizer = regularizers.l2(l2_reg) if l2_reg > 0.0 else None
                   ),
      layers.Dropout(dropout_rate),
      layers.Dense(100, activation=activation,
                   kernel_regularizer = regularizers.l2(l2_reg) if l2_reg > 0.0 else None
                   ),
      layers.Dropout(dropout_rate),
      layers.Dense(10, activation='softmax')
  ])

  model.compile(
      loss='sparse_categorical_crossentropy',
      optimizer=optimizer,
      metrics=['accuracy']
    )

  return model

from re import VERBOSE
activations =  ['relu', 'tanh', 'sigmoid']
results_activations = {}

for act in activations:
  print(f"n----Training with activation = {act}---")
  model = build_mlp(activation=act)
  history = model.fit(
      X_train, y_train,
      epochs=5, batch_size=128,
      validation_data=(X_valid, y_valid),
      verbose=0
  )

  val_acc =  history.history['val_accuracy'][-1]
  results_activations[act] = val_acc
  print(f"Validation accuracy with {act} activation: {val_acc:.4f}")

print(results_activations)

optimizers = {
    'adam' : Adam(learning_rate=0.001),
    'sgd' : SGD(learning_rate=0.01, momentum=0.9),
    'rmsprop' : RMSprop(learning_rate=0.001)
}

results_opt = {}

for name, opt in optimizers.items():
  print(f"Training with optimizer: {name}")
  model = build_mlp(activation='relu', optimizer=opt)
  history = model.fit(X_train, y_train,
                     epochs=5, batch_size=128,
                      validation_data=(X_valid, y_valid),
                      verbose=0
                      )
  val_acc = history.history['val_accuracy'][-1]
  results_opt[name] = val_acc

print(results_opt)

regularization_settings = [
    {'dropout_rate' : 0.0, 'l2_reg': 0.0},
    {'dropout_rate' : 0.3, 'l2_reg': 0.0},
    {'dropout_rate' : 0.5, 'l2_reg': 0.001},
]

results_reg = {}

for settings in regularization_settings:
  dr, l2r = settings['dropout_rate'], settings['l2_reg']
  print(f"\n---Training with dropout_rate = {dr} and l2_reg = {l2r}---")
  model = build_mlp(dropout_rate=dr, l2_reg=l2r)
  history = model.fit(X_train, y_train,
                      epochs=5, batch_size=128,
                      validation_data=(X_valid, y_valid),
                      verbose=0
                      )
  val_acc = history.history['val_accuracy'][-1]
  results_reg[f"dropout_rate={dr}, l2_reg={l2r}"] = val_acc
  print(f"Validation accuracy with dropout_rate={dr}, l2_reg={l2r}: {val_acc:.4f}")

print(results_reg)

def build_mlp_depth(depth=2):
  model = keras.Sequential([layers.Flatten(input_shape=[28, 28])])
  for i in range(depth):
    model.add(layers.Dense(128, activation='relu'))
  model.add(layers.Dense(10, activation='softmax'))
  model.compile(loss="sparse_categorical_crossentropy",
                optimizer='adam',
                metrics=['accuracy'])

  return model



depths = [1, 2, 3, 4]
results_depth = {}

for d in depths:
  print(f"n-----Training with depth = {d} hidden layers-----")
  model = build_mlp_depth(depth=d)
  history = model.fit(X_train, y_train,
                      epochs=5, batch_size=128,
                      validation_data=(X_valid, y_valid),
                      verbose=0
  )
  val_acc = history.history['val_accuracy'][-1]
  results_depth[d] = val_acc
  print(f"Validation accuracy with {d} hidden layers: {val_acc:.4f}")

print("n\ Depth comparison:", results_depth)

def build_cnn(filters1=32, filters2=64, dropout_rate=0.5, activation='relu'):
    model = keras.Sequential([
        layers.Conv2D(filters1, (3,3), activation=activation, padding='same', input_shape=(28,28,1)),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(filters2, (3,3), activation=activation, padding='same'),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dense(128, activation=activation),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

cnn_settings = [
    {'filters1': 32, 'filters2': 64, 'dropout_rate': 0.5},
    {'filters1': 64, 'filters2': 128, 'dropout_rate': 0.3},
    {'filters1': 16, 'filters2': 32, 'dropout_rate': 0.0}
]

results_cnn = {}

for s in cnn_settings:
    print(f"\n--- CNN with filters ({s['filters1']},{s['filters2']}), dropout={s['dropout_rate']} ---")
    model = build_cnn(**s)
    history = model.fit(
        X_train_cnn, y_train,
        epochs=5, batch_size=128,
        validation_data=(X_valid_cnn, y_valid),
        verbose=0
    )
    val_acc = history.history['val_accuracy'][-1]
    results_cnn[str(s)] = val_acc
    print(f"Validation accuracy: {val_acc:.4f}")

print("\nCNN configuration comparison:", results_cnn)

print("=== Summary of MLP Experiments ===")
print("Activation:", results_activations)
print("Optimizers:", results_opt)
print("Regularization:", results_reg)
print("Depth:", results_depth)

print("\n=== CNN Experiments ===")
print(results_cnn)

from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical

# Load dataset
(X_train_full, y_train_full), (X_test, y_test) = cifar10.load_data()

# Normalize to [0,1]
X_train_full = X_train_full.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# Create validation split (10% of training)
num_val = int(0.1 * X_train_full.shape[0])
X_valid, X_train = X_train_full[:num_val], X_train_full[num_val:]
y_valid, y_train = y_train_full[:num_val], y_train_full[num_val:]

print("Train:", X_train.shape, y_train.shape)
print("Valid:", X_valid.shape, y_valid.shape)
print("Test :", X_test.shape, y_test.shape)

def build_mlp_cifar(activation='relu', dropout_rate=0.3):    model = keras.Sequential([
        layers.Flatten(input_shape=(32, 32, 3)),
        layers.Dense(512, activation=activation),
        layers.Dropout(dropout_rate),
        layers.Dense(256, activation=activation),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model


def build_cnn_cifar(filters1=32, filters2=64, dropout_rate=0.5, activation='relu'):
    model = keras.Sequential([
        layers.Conv2D(filters1, (3,3), activation=activation, padding='same', input_shape=(32,32,3)),
        layers.Conv2D(filters1, (3,3), activation=activation, padding='same'),
        layers.MaxPooling2D((2,2)),
        layers.Conv2D(filters2, (3,3), activation=activation, padding='same'),
        layers.Conv2D(filters2, (3,3), activation=activation, padding='same'),
        layers.MaxPooling2D((2,2)),
        layers.Flatten(),
        layers.Dense(256, activation=activation),
        layers.Dropout(dropout_rate),
        layers.Dense(10, activation='softmax')
    ])
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

mlp_cifar = build_mlp_cifar(activation='relu', dropout_rate=0.3)

history_mlp_cifar = mlp_cifar.fit(
    X_train, y_train,
    epochs=15,
    batch_size=128,
    validation_data=(X_valid, y_valid)
)

test_loss_mlp, test_acc_mlp = mlp_cifar.evaluate(X_test, y_test)
print(f"Test accuracy (MLP on CIFAR-10): {test_acc_mlp:.4f}")

cnn_cifar = build_cnn_cifar(filters1=64, filters2=128, dropout_rate=0.3)

history_cnn_cifar = cnn_cifar.fit(
    X_train, y_train,
    epochs=15,
    batch_size=128,
    validation_data=(X_valid, y_valid)
)

test_loss_cnn, test_acc_cnn = cnn_cifar.evaluate(X_test, y_test)
print(f"Test accuracy (CNN on CIFAR-10): {test_acc_cnn:.4f}")

plt.figure(figsize=(8,5))
plt.plot(history_mlp_cifar.history['val_accuracy'], label='MLP val_acc')
plt.plot(history_cnn_cifar.history['val_accuracy'], label='CNN val_acc')
plt.title('Validation Accuracy on CIFAR-10')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()