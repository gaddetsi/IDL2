{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6578d83f",
   "metadata": {},
   "source": [
    "# Code for TASK 2.1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5df11401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, Input, BatchNormalization, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework.ops import SymbolicTensor\n",
    "from keras.src import ops\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "import os\n",
    "\n",
    "from get_dataset import download_data\n",
    "# from task2_1_a import preprocess_cat, print_metrics, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a633ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 24\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 1000 # we use early stopping, so not all epochs will be used\n",
    "\n",
    "def print_metrics(pred_hours, true_hours, model_name, num_classes=NUM_CLASSES):\n",
    "    \"\"\"Print comprehensive evaluation metrics.\"\"\"\n",
    "    diff_min = common_sense_categories_loss(true_hours,pred_hours) #returns common sense loss in minutes from one hot encoded labels\n",
    "    \n",
    "    mean_err = np.mean(diff_min)\n",
    "    median_err = np.median(diff_min)\n",
    "    std_err = np.std(diff_min)\n",
    "    max_err = np.max(diff_min)\n",
    "    \n",
    "    within_0 = np.mean(diff_min <= 0) * 100\n",
    "    within_1 = np.mean(diff_min <= 1) * 100\n",
    "    within_5 = np.mean(diff_min <= 5) * 100\n",
    "    within_10 = np.mean(diff_min <= 10) * 100\n",
    "    within_15 = np.mean(diff_min <= 15) * 100\n",
    "    within_30 = np.mean(diff_min <= 30) * 100\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{model_name} - TEST SET RESULTS\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Mean Absolute Common Sense Loss:    {mean_err:.2f} minutes\")\n",
    "    print(f\"Median Absolute Common Sense Loss:  {median_err:.2f} minutes\")\n",
    "    print(f\"Std Deviation:                      {std_err:.2f} minutes\")\n",
    "    print(f\"Max Common Sense Loss:              {max_err:.2f} minutes\")\n",
    "    print(f\"\\nAccuracy within thresholds:\")\n",
    "    print(f\"  Within 30 minutes:                {within_30:.1f}%\")\n",
    "    print(f\"  Within 15 minutes:                {within_15:.1f}%\")\n",
    "    print(f\"  Within 10 minutes:                {within_10:.1f}%\")\n",
    "    print(f\"  Within 5 minutes:                 {within_5:.1f}%\")\n",
    "    print(f\"  Within 1 minute:                  {within_1:.1f}%\")\n",
    "    print(f\"  Accuracy:                         {within_0:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'mean': mean_err,\n",
    "        'median': median_err,\n",
    "        'std': std_err,\n",
    "        'max': max_err,\n",
    "        'accuracy': within_0,\n",
    "        'within_1': within_1,\n",
    "        'within_5': within_5,\n",
    "        'within_10': within_10,\n",
    "        'within_15': within_15,\n",
    "        'within_30': within_30,\n",
    "        'predictions': pred_hours,\n",
    "        'errors': diff_min\n",
    "    }\n",
    "\n",
    "def to_categorical(y, num_classes):\n",
    "    \"\"\"\n",
    "    --------------------------------------------\n",
    "    Convert time to categorical labels.\n",
    "    --------------------------------------------\n",
    "    :param y: numpy array of shape (num_samples, 2) where the first column is hour (0-11)\n",
    "              and the second column is minute (0-59)\n",
    "    :param num_classes: total number of classes\n",
    "    :return: numpy array with one-hot encoded labels that represent the class that the elements of y belong to\n",
    "    \"\"\"\n",
    "    # check if it is already one-hot encoded\n",
    "    if y.shape[1] == num_classes:\n",
    "        return y\n",
    "\n",
    "    class_ph = num_classes//12                    # amount of classes per hour\n",
    "    class_pm = 60//class_ph                       # amount of classes per minute\n",
    "    class_ = y[:, 0]*class_ph + y[:, 1]//class_pm # if num_class is 24, then per 30 min is the next class.\n",
    "\n",
    "    return keras.utils.to_categorical(class_, num_classes)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def common_sense_categories_loss(y_true: SymbolicTensor, y_pred: SymbolicTensor, num_classes = NUM_CLASSES) -> SymbolicTensor:\n",
    "    \"\"\"\n",
    "    --------------------------------------------\n",
    "    Get common sense loss for categories\n",
    "    --------------------------------------------\n",
    "    :param y_true: tensor filled with tensors with true labels\n",
    "    :param y_pred: tensor filled with tensors with predicted labels\n",
    "    :return: accuracy loss tensor\n",
    "\n",
    "    Common sense loss formula: \n",
    "    (highest possible common sence loss) = cls = min(|true_class-pred_class|, ||true_class-pred_class| - number_of_classes|)\n",
    "    csl*category_size_in_minutes = common sense loss in minutes\n",
    "    \"\"\"\n",
    "    # read what the class is\n",
    "    y_true = tf.argmax(y_true, axis=-1) # when printed gives: tf.Tensor(0, shape=(), dtype=int64), here it gives 0 because it is class 0\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "\n",
    "    #calc common sense loss (cls)\n",
    "    diff = tf.abs(y_true - y_pred)                        # difference (not common sence yet)\n",
    "    csl = tf.minimum(diff, tf.abs(diff - num_classes))    # highest possible common sence loss (=common sence difference)\n",
    "    minutes_per_class = 60//(num_classes//12)             # how many minutes per class\n",
    "    return csl * minutes_per_class                        # return common sense loss in minutes\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def common_sense_mse(y_true,y_pred, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    --------------------------------------------\n",
    "    Get mean squared error with common sense\n",
    "    --------------------------------------------\n",
    "    :param y_true: tensor filled with tensors with true labels\n",
    "    :param y_pred: tensor filled with tensors with predicted labels\n",
    "    :return: mean squared error (MSE) with common sense incorporated (MSE, but we make classes that are further away have more loss)\n",
    "\n",
    "    Make a distribution that is bigger in value when farther away from the correct class. \n",
    "    That distribution is then used as weights to alter the mean squared error (MSE).\n",
    "    What is returned is: MSE * distribution\n",
    "    \"\"\"\n",
    "    # precausion, make them have the same dtype\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # get squared difference\n",
    "    diff = tf.square(y_true - y_pred)  # shape (batch, num_classes)\n",
    "\n",
    "    # get the class\n",
    "    class_by_mult = tf.range(num_classes, dtype=tf.float32)\n",
    "    true_class = tf.reduce_sum(y_true * class_by_mult, axis=-1) \n",
    "    true_class = tf.cast(true_class, tf.int32)  # must be int for tf.roll\n",
    "    \n",
    "    # get distribution based on absolute distance for when index = 0\n",
    "    dist = [1]\n",
    "    max_amount = np.ceil(num_classes / 2)\n",
    "    for i in range(2,int(max_amount)+1):\n",
    "        dist.append(i)\n",
    "    for i in range(int(max_amount)+1,1,-1):\n",
    "        dist.append(i)\n",
    "    dist = tf.constant(dist, dtype=tf.float32)\n",
    "\n",
    "    # rotate distribution so that value 1 aligns with the true class\n",
    "    # Ex: when index=0 is the correct class, then the distribution \n",
    "    # for 4 classes would look like this: [1,2,3,2]\n",
    "    # , when index = 1 it would look like this: [2,1,2,3]\n",
    "    def rotate_dist(idx):\n",
    "        idx = tf.reshape(idx, ())  # make is a scalar\n",
    "        row = tf.roll(dist, shift=idx, axis=0)\n",
    "        return row\n",
    "\n",
    "    dist_matrix = tf.map_fn(rotate_dist, true_class, fn_output_signature=tf.float32) # loop over true_class rows\n",
    "\n",
    "    # multiply squared difference by distance weights\n",
    "    diff_with_common_sense = diff * dist_matrix\n",
    "\n",
    "    # average over classes to get the MSE with common_sense incorporated\n",
    "    loss = tf.reduce_mean(diff_with_common_sense, axis=-1)\n",
    "    return loss\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def common_sense_mse_720(y_true,y_pred):\n",
    "    return common_sense_mse(y_true,y_pred, num_classes=720)\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def common_sense_mse_0(y_true,y_pred, num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    --------------------------------------------\n",
    "    Get mean squared error with common sense\n",
    "    --------------------------------------------\n",
    "    :param y_true: tensor filled with tensors with true labels\n",
    "    :param y_pred: tensor filled with tensors with predicted labels\n",
    "    :return: mean squared error (MSE) with common sense incorporated (MSE, but we make classes that are further away have more loss)\n",
    "\n",
    "    Make a distribution that is bigger in value when farther away from the correct class. \n",
    "    That distribution is then used as weights to alter the mean squared error (MSE).\n",
    "    What is returned is: MSE * distribution\n",
    "    \"\"\"\n",
    "    # precausion, make them have the same dtype\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # get squared difference\n",
    "    diff = tf.abs(y_true - y_pred)  # shape (batch, num_classes)\n",
    "\n",
    "    # get the class\n",
    "    class_by_mult = tf.range(num_classes, dtype=tf.float32)\n",
    "    true_class = tf.reduce_sum(y_true * class_by_mult, axis=-1) \n",
    "    true_class = tf.cast(true_class, tf.int32)  # must be int for tf.roll\n",
    "    \n",
    "    # get distribution based on absolute distance for when index = 0\n",
    "    dist = [0]\n",
    "    max_amount = np.ceil(num_classes / 2)\n",
    "    for i in range(1,int(max_amount)):\n",
    "        dist.append(i)\n",
    "    for i in range(int(max_amount),0,-1):\n",
    "        dist.append(i)\n",
    "    dist = tf.constant(dist, dtype=tf.float32)\n",
    "\n",
    "    # rotate distribution so that value 0 aligns with the true class\n",
    "    # Ex: when index=0 is the correct class, then the distribution \n",
    "    # for 4 classes would look like this: [0,1,2,1]\n",
    "    # , when index = 1 it would look like this: [1,0,1,2]\n",
    "    def rotate_dist(idx):\n",
    "        idx = tf.reshape(idx, ())  # make it a scalar\n",
    "        row = tf.roll(dist, shift=idx, axis=0)\n",
    "        return row\n",
    "\n",
    "    dist_matrix = tf.map_fn(rotate_dist, true_class, fn_output_signature=tf.float32) # loop over true_class rows\n",
    "\n",
    "    # multiply squared difference by distance weights\n",
    "    diff_with_common_sense = diff * dist_matrix\n",
    "\n",
    "    # average over classes to get the MSE with common_sense incorporated\n",
    "    loss = tf.reduce_mean(tf.square(diff_with_common_sense), axis=-1)\n",
    "    return loss\n",
    "\n",
    "def load_data(seed: int,easy=True) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, tuple[int, int, int]]:\n",
    "    url = r\"https://surfdrive.surf.nl/index.php/s/Nznt5c48Mzlb2HY/download?path=%2F&files=A1_data_75.zip\"\n",
    "    download_data(url)\n",
    "\n",
    "    url = r\"https://surfdrive.surf.nl/index.php/s/Nznt5c48Mzlb2HY/download?path=%2F&files=A1_data_150.zip\"\n",
    "    download_data(url)\n",
    "    # choose dataset\n",
    "    if easy:\n",
    "        X = np.load(\"data/A1_data_75/images.npy\")\n",
    "        y = np.load(\"data/A1_data_75/labels.npy\")\n",
    "    else:\n",
    "        X = np.load(\"data/A1_data_150/images.npy\")\n",
    "        y = np.load(\"data/A1_data_150/labels.npy\")\n",
    "\n",
    "    X = X / 255\n",
    "    # Split into train/val/test (80/10/10)\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.10, random_state=seed, shuffle=True\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.1111, random_state=seed, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Add channel dimension for CNN\n",
    "    img_rows, img_cols = X_train.shape[1], X_train.shape[2]\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "        X_val = X_val.reshape(X_val.shape[0], 1, img_rows, img_cols)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "        X_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, input_shape\n",
    "\n",
    "\n",
    "def build_cnn_catagorical(input_shape, num_classes):\n",
    "    \"\"\"CNN for predicting categorical values.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # First conv block\n",
    "    x = Conv2D(32, (3, 3), padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Second conv block\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Third conv block\n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Dense layers\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # Output layer: num_classes amount of nodes\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name=\"cnn_classification\")\n",
    "\n",
    "\n",
    "def preprocess_cat(easy=True, num_classes=NUM_CLASSES):\n",
    "    # preprocessing categorical data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, input_shape = load_data(seed=42, easy=easy)\n",
    "\n",
    "    # Convert labels to one-hot encoding given the number of classes\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_val = to_categorical(y_val, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, input_shape\n",
    "\n",
    "\n",
    "def experiment_cat(loss_function, model_name: str, easy=True, num_classes=NUM_CLASSES):\n",
    "    \"\"\"-------------------\n",
    "    Run experiment for categorical models, saves models that can be used to plot the graphs.\n",
    "    -------------------\n",
    "    :param loss_function: loss function to use\n",
    "    :param model_name: name of the model (for saving purposes)\n",
    "    :param easy: whether to use the easy dataset or hard dataset\n",
    "    \"\"\"\n",
    "    # set seeds\n",
    "    seed=42\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    keras.utils.set_random_seed(seed)\n",
    "\n",
    "    # load preprocessed data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, input_shape = preprocess_cat(easy,num_classes)\n",
    "\n",
    "    # init model\n",
    "    model = build_cnn_catagorical(input_shape, num_classes)\n",
    "    \n",
    "\n",
    "    # use a chosen loss function and common sense loss as metric and init optimizer and learning rate\n",
    "    model.compile(loss=loss_function,\n",
    "                optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                metrics=[common_sense_categories_loss])\n",
    "    \n",
    "    # show model information\n",
    "    model.summary()\n",
    "\n",
    "    # make callback that lowers the learning rate when a plateau is reached and has early stopping\n",
    "    callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss', patience=5, factor=0.5, verbose=1, min_lr=1e-7\n",
    "        ),\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, verbose=1, restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "    # train model\n",
    "    model.fit(X_train, y_train,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=EPOCHS,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks,\n",
    "            validation_data=(X_val, y_val))\n",
    "    \n",
    "    # save model\n",
    "    model.save(f'saved_models/loss_{model_name}.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bfd30b",
   "metadata": {},
   "source": [
    "# RUN experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d63ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "# run experiments for different loss functions and the hard dataset (150 dataset)\n",
    "experiment_cat(loss_function=common_sense_mse, model_name=\"common_sense_mse\", easy=False)\n",
    "experiment_cat(loss_function=common_sense_mse_0, model_name=\"common_sense_mse_0\", easy=False)\n",
    "experiment_cat(loss_function=keras.losses.MSE, model_name=\"mse\", easy=False)\n",
    "experiment_cat(loss_function=keras.losses.categorical_crossentropy, model_name=\"crossentropy\", easy=False)\n",
    "\n",
    "# run experiments for different loss functions and the hard dataset\n",
    "experiment_cat(loss_function=common_sense_mse_720, model_name=\"common_sense_mse_hard\", easy=False, num_classes=720)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46534ce",
   "metadata": {},
   "source": [
    "# Save plots in images/categorical and print metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cbc3ba",
   "metadata": {},
   "source": [
    "give what models to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b1f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    model_list= [\"common_sense_mse\",\"common_sense_mse_0\",\"mse\",\"crossentropy\",\"common_sense_mse_hard\"]\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978654f9",
   "metadata": {},
   "source": [
    "plot models and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d6bd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file does not exist\n",
      "file does not exist\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.72 GiB for an array with shape (16200, 150, 150) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     51\u001b[39m     easy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# load preprocessed data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m X_train, y_train, X_val, y_val, X_test, y_test, input_shape = \u001b[43mpreprocess_cat\u001b[49m\u001b[43m(\u001b[49m\u001b[43measy\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# convert y_test to categorical labels\u001b[39;00m\n\u001b[32m     56\u001b[39m y_test = to_categorical(y_test, num_classes)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 298\u001b[39m, in \u001b[36mpreprocess_cat\u001b[39m\u001b[34m(easy, num_classes)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpreprocess_cat\u001b[39m(easy=\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes=NUM_CLASSES):\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m# preprocessing categorical data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     X_train, y_train, X_val, y_val, X_test, y_test, input_shape = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43measy\u001b[49m\u001b[43m=\u001b[49m\u001b[43measy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m     \u001b[38;5;66;03m# Convert labels to one-hot encoding given the number of classes\u001b[39;00m\n\u001b[32m    301\u001b[39m     y_train = to_categorical(y_train, num_classes)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(seed, easy)\u001b[39m\n\u001b[32m    220\u001b[39m X = X / \u001b[32m255\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;66;03m# Split into train/val/test (80/10/10)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m X_train_full, X_test, y_train_full, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    224\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m X_train, X_val, y_train, y_val = train_test_split(\n\u001b[32m    226\u001b[39m     X_train_full, y_train_full, test_size=\u001b[32m0.1111\u001b[39m, random_state=seed, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    227\u001b[39m )\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# Add channel dimension for CNN\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nooit\\OneDrive\\Documenten\\Datascience and Artificialinteligence\\MasterCS Jaar1\\IDL\\intro deeplearning ass1\\IDL2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nooit\\OneDrive\\Documenten\\Datascience and Artificialinteligence\\MasterCS Jaar1\\IDL\\intro deeplearning ass1\\IDL2\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2944\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2940\u001b[39m     train, test = \u001b[38;5;28mnext\u001b[39m(cv.split(X=arrays[\u001b[32m0\u001b[39m], y=stratify))\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2946\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2948\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nooit\\OneDrive\\Documenten\\Datascience and Artificialinteligence\\MasterCS Jaar1\\IDL\\intro deeplearning ass1\\IDL2\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2946\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   2940\u001b[39m     train, test = \u001b[38;5;28mnext\u001b[39m(cv.split(X=arrays[\u001b[32m0\u001b[39m], y=stratify))\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m-> \u001b[39m\u001b[32m2946\u001b[39m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nooit\\OneDrive\\Documenten\\Datascience and Artificialinteligence\\MasterCS Jaar1\\IDL\\intro deeplearning ass1\\IDL2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:349\u001b[39m, in \u001b[36m_safe_indexing\u001b[39m\u001b[34m(X, indices, axis)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m warnings.warn(\n\u001b[32m    342\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[33mA data object with support for the dataframe interchange protocol\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwas passed, but scikit-learn does currently not know how to handle this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkind of data. Some array/list indexing will be tried.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    345\u001b[39m         category=\u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    346\u001b[39m     )\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nooit\\OneDrive\\Documenten\\Datascience and Artificialinteligence\\MasterCS Jaar1\\IDL\\intro deeplearning ass1\\IDL2\\.venv\\Lib\\site-packages\\sklearn\\utils\\_indexing.py:40\u001b[39m, in \u001b[36m_array_indexing\u001b[39m\u001b[34m(array, key, key_dtype, axis)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m     39\u001b[39m     key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 2.72 GiB for an array with shape (16200, 150, 150) and data type float64"
     ]
    }
   ],
   "source": [
    "# EXAMPLE USAGE: python ./create_plots_cat.py ./saved_models/loss_*\n",
    "\n",
    "# RUN THIS TO CREATE PLOTS FOR CATEGORICAL MODELS AND PRINT METRICS\n",
    "\n",
    "def plot_error_histogram(errors, model_name, save_path=None):\n",
    "    \"\"\"Plot histogram of prediction errors.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=50, edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Absolute Error (minutes)', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.title(f'{model_name} - Error Distribution', fontsize=14, fontweight='bold')\n",
    "    plt.axvline(np.mean(errors), color='r', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.1f} min')\n",
    "    plt.axvline(np.median(errors), color='g', linestyle='--', linewidth=2, label=f'Median: {np.median(errors):.1f} min')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions_vs_true(pred_classes, true_classes, model_name, save_path=None):\n",
    "    \"\"\"Scatter plot of predictions vs true values.\"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(true_classes/(24/12), pred_classes/(24/12), alpha=0.3, s=10)\n",
    "    plt.plot([0, 12], [0, 12], 'r--', linewidth=2, label='Perfect prediction')\n",
    "    plt.xlabel('True Time (hours)', fontsize=12)\n",
    "    plt.ylabel('Predicted Time (hours)', fontsize=12)\n",
    "    plt.title(f'{model_name} - Predictions vs True Values', fontsize=14, fontweight='bold')\n",
    "    plt.xlim([0, 12])\n",
    "    plt.ylim([0, 12])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create output directory if it doesn't exist\n",
    "    dir = f\"./images/categorical\"\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    # load model\n",
    "    for curr_model in get_models():\n",
    "        # select parameters\n",
    "        model_path = f\"./saved_models/loss_{curr_model}.keras\"\n",
    "        if curr_model.endswith(\"hard\"):\n",
    "            num_classes = 720\n",
    "            easy = False\n",
    "        else:\n",
    "            num_classes = 24\n",
    "            easy = False\n",
    "\n",
    "        # load preprocessed data\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test, input_shape = preprocess_cat(easy,num_classes)\n",
    "        # convert y_test to categorical labels\n",
    "        y_test = to_categorical(y_test, num_classes)\n",
    "        # load model\n",
    "        model = keras.models.load_model(model_path)\n",
    "        # make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        # print metrics\n",
    "        metrics = print_metrics(y_test, y_pred, curr_model, num_classes)\n",
    "        # get common sense loss\n",
    "        errors = metrics['errors']\n",
    "\n",
    "        # plot error histogram\n",
    "        plot_error_histogram(errors, model_name=curr_model, save_path=f\"{dir}/{curr_model}_error_histogram.png\")\n",
    "\n",
    "        # plot predictions vs true values\n",
    "        plot_predictions_vs_true(np.argmax(y_pred, axis=1), np.argmax(y_test, axis=1), model_name=curr_model, save_path=f\"{dir}/{curr_model}_pred_vs_true.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
